本实验采用机器学习和深度学习方法对汉语篇章小句进行切分。把逗号和句号作为句子的候选切分点，把已标注好子句边界信息的CTB6.0中前100篇文档作为训练集和验证集。在使用机器学习方法分类前，采用词袋模型和TF-IDF模型对每一个候选句子切分点提取其上下文特征。由于训练集小句共有3066个，整个文集中不同词语的数量非常大，而单个文件使用的词语数量远远少于文集的词语个数，所以采用词袋模型和TF-IDF模型构造的向量维度较大而且每个向量中大多数的值为0，是高维稀疏数据。此时采用KNN、决策树、支持向量机、神经网络等分类模型会耗费大量的机器内存和运算时间。所以，本实验选择了朴素贝叶斯模型和逻辑回归模型对候选切分点进行分类。再使用深度学习在自然语言处理领域的新兴模型BERT针对以上方法暴露的缺点做出改进。

编程语言：python
运行环境：python3.7

语料处理：(按顺序跑，并按注释指示修改参数或文件名）
xml-txt.py--从xml文件提取文本到txt文件中
txt-json.py--对上述得到的txt文件处理后写入json文件
txt-csv.py--对上述得到的txt文件处理后写入csv文件

机器学习模型：
bag_sklearn.py--词袋模型特征提取-机器学习分类
tfidf_sklearn.py--TF-IDF模型特征提取-机器学习分类

Bert模型：(按顺序跑，并按注释指示修改参数或文件名）
train.py--建立模型
demo.py--句子切分结果
